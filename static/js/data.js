window.CONTENTBENCH_DATA = 
{
  "generated": "2026-02-08T02:09:02.751392",
  "dataset_info": {
    "total_posts": 1000,
    "categories": 5
  },
  "models": [
    {
      "model_name": "gemini/gemini-2.5-flash-preview-09-2025",
      "display_name": "Gemini: Gemini 2.5 Flash Preview (09-2025)",
      "accuracy": 99.8,
      "sarcasm_recall": 100.0,
      "cost_per_50k": 5.08,
      "developer": "Google",
      "rank": 1
    },
    {
      "model_name": "gemini/gemini-2.5-flash",
      "display_name": "Gemini: Gemini 2.5 Flash",
      "accuracy": 99.6,
      "sarcasm_recall": 100.0,
      "cost_per_50k": 5.08,
      "developer": "Google",
      "rank": 2
    },
    {
      "model_name": "gemini/gemini-2.5-flash-lite-preview-09-2025",
      "display_name": "Gemini: Gemini 2.5 Flash Lite Preview (09-2025)",
      "accuracy": 99.4,
      "sarcasm_recall": 99.6,
      "cost_per_50k": 1.57,
      "developer": "Google",
      "rank": 3
    },
    {
      "model_name": "gemini/gemini-2.0-flash-001",
      "display_name": "Gemini: Gemini 2.0 Flash",
      "accuracy": 99.2,
      "sarcasm_recall": 100.0,
      "cost_per_50k": 1.57,
      "developer": "Google",
      "rank": 4
    },
    {
      "model_name": "openai/gpt-5-mini",
      "display_name": "OpenAI: GPT-5 Mini",
      "accuracy": 99.0,
      "sarcasm_recall": 99.0,
      "cost_per_50k": 5.03,
      "developer": "OpenAI",
      "rank": 5
    },
    {
      "model_name": "openrouter/z-ai/glm-4-32b",
      "display_name": "Z.AI: GLM 4 32B",
      "accuracy": 98.7,
      "sarcasm_recall": 98.8,
      "cost_per_50k": 1.47,
      "developer": "Zhipu",
      "rank": 6
    },
    {
      "model_name": "openrouter/meta-llama/llama-4-maverick",
      "display_name": "Meta: Llama 4 Maverick",
      "accuracy": 98.4,
      "sarcasm_recall": 100.0,
      "cost_per_50k": 2.4,
      "developer": "Meta",
      "rank": 7
    },
    {
      "model_name": "openrouter/qwen/qwen3-235b-a22b-2507",
      "display_name": "Qwen: Qwen3 235B A22B Instruct 2507",
      "accuracy": 98.4,
      "sarcasm_recall": 99.8,
      "cost_per_50k": 1.5,
      "developer": "Alibaba",
      "rank": 8
    },
    {
      "model_name": "gemini/gemini-2.0-flash-lite-001",
      "display_name": "Gemini: Gemini 2.0 Flash Lite",
      "accuracy": 98.0,
      "sarcasm_recall": 100.0,
      "cost_per_50k": 1.18,
      "developer": "Google",
      "rank": 9
    },
    {
      "model_name": "openrouter/google/gemini-flash-1.5",
      "display_name": "Google: Gemini 1.5 Flash",
      "accuracy": 97.2,
      "sarcasm_recall": 96.2,
      "cost_per_50k": 1.18,
      "developer": "Google",
      "rank": 10
    },
    {
      "model_name": "gemini/gemma-3-27b-it",
      "display_name": "Gemini: Gemma 3 27B",
      "accuracy": 97.1,
      "sarcasm_recall": 96.6,
      "cost_per_50k": null,
      "developer": "Google",
      "rank": 11
    },
    {
      "model_name": "openrouter/meta-llama/llama-4-scout",
      "display_name": "Meta: Llama 4 Scout",
      "accuracy": 96.9,
      "sarcasm_recall": 97.8,
      "cost_per_50k": 1.27,
      "developer": "Meta",
      "rank": 12
    },
    {
      "model_name": "openrouter/meituan/longcat-flash-chat",
      "display_name": "Meituan: LongCat Flash Chat",
      "accuracy": 96.6,
      "sarcasm_recall": 94.2,
      "cost_per_50k": 2.5,
      "developer": "Meituan",
      "rank": 13
    },
    {
      "model_name": "openrouter/qwen/qwen3-next-80b-a3b-instruct",
      "display_name": "Qwen: Qwen3 Next 80B A3B Instruct",
      "accuracy": 96.6,
      "sarcasm_recall": 95.6,
      "cost_per_50k": 2.38,
      "developer": "Alibaba",
      "rank": 14
    },
    {
      "model_name": "openrouter/qwen/qwen-2.5-72b-instruct",
      "display_name": "Qwen2.5 72B Instruct",
      "accuracy": 96.1,
      "sarcasm_recall": 95.0,
      "cost_per_50k": 1.88,
      "developer": "Alibaba",
      "rank": 15
    },
    {
      "model_name": "gemini/gemini-2.5-flash-lite",
      "display_name": "Gemini: Gemini 2.5 Flash Lite",
      "accuracy": 96.1,
      "sarcasm_recall": 95.0,
      "cost_per_50k": 1.57,
      "developer": "Google",
      "rank": 16
    },
    {
      "model_name": "openrouter/meta-llama/llama-3.1-70b-instruct",
      "display_name": "Meta: Llama 3.1 70B Instruct",
      "accuracy": 95.2,
      "sarcasm_recall": 92.8,
      "cost_per_50k": 1.55,
      "developer": "Meta",
      "rank": 17
    },
    {
      "model_name": "openrouter/meta-llama/llama-3.3-70b-instruct",
      "display_name": "Meta: Llama 3.3 70B Instruct",
      "accuracy": 94.7,
      "sarcasm_recall": 91.6,
      "cost_per_50k": 0.2,
      "developer": "Meta",
      "rank": 18
    },
    {
      "model_name": "openrouter/nousresearch/hermes-3-llama-3.1-70b",
      "display_name": "Nous: Hermes 3 70B Instruct",
      "accuracy": 93.8,
      "sarcasm_recall": 90.0,
      "cost_per_50k": 1.85,
      "developer": "Nous Research",
      "rank": 19
    },
    {
      "model_name": "openrouter/qwen/qwen3-30b-a3b-instruct-2507",
      "display_name": "Qwen: Qwen3 30B A3B Instruct 2507",
      "accuracy": 91.0,
      "sarcasm_recall": 86.4,
      "cost_per_50k": 1.43,
      "developer": "Alibaba",
      "rank": 20
    },
    {
      "model_name": "openrouter/google/gemini-flash-1.5-8b",
      "display_name": "Google: Gemini 1.5 Flash 8B",
      "accuracy": 89.0,
      "sarcasm_recall": 86.2,
      "cost_per_50k": 0.6,
      "developer": "Google",
      "rank": 21
    },
    {
      "model_name": "openrouter/mistralai/mistral-small-3.2-24b-instruct",
      "display_name": "Mistral: Mistral Small 3.2 24B",
      "accuracy": 84.1,
      "sarcasm_recall": 74.6,
      "cost_per_50k": 1.57,
      "developer": "Mistral",
      "rank": 22
    },
    {
      "model_name": "openrouter/mistralai/mistral-small-3.1-24b-instruct",
      "display_name": "Mistral: Mistral Small 3.1 24B",
      "accuracy": 82.7,
      "sarcasm_recall": 72.4,
      "cost_per_50k": 1.57,
      "developer": "Mistral",
      "rank": 23
    },
    {
      "model_name": "openrouter/mistralai/mistral-7b-instruct",
      "display_name": "Mistral: Mistral 7B Instruct",
      "accuracy": 80.5,
      "sarcasm_recall": 70.0,
      "cost_per_50k": 2.0,
      "developer": "Mistral",
      "rank": 24
    },
    {
      "model_name": "openrouter/mistralai/devstral-small",
      "display_name": "Mistral: Devstral Small 1.1",
      "accuracy": 80.3,
      "sarcasm_recall": 68.2,
      "cost_per_50k": 1.12,
      "developer": "Mistral",
      "rank": 25
    },
    {
      "model_name": "openrouter/amazon/nova-micro-v1",
      "display_name": "Amazon: Nova Micro 1.0",
      "accuracy": 76.6,
      "sarcasm_recall": 64.8,
      "cost_per_50k": 0.53,
      "developer": "Amazon",
      "rank": 26
    },
    {
      "model_name": "openrouter/mistralai/devstral-small-2505",
      "display_name": "Mistral: Devstral Small 2505",
      "accuracy": 72.0,
      "sarcasm_recall": 53.2,
      "cost_per_50k": 0.93,
      "developer": "Mistral",
      "rank": 27
    },
    {
      "model_name": "gemini/gemma-3-12b-it",
      "display_name": "Gemini: Gemma 3 12B",
      "accuracy": 70.6,
      "sarcasm_recall": 53.4,
      "cost_per_50k": null,
      "developer": "Google",
      "rank": 28
    },
    {
      "model_name": "openrouter/meta-llama/llama-3.1-8b-instruct",
      "display_name": "Meta: Llama 3.1 8B Instruct",
      "accuracy": 69.4,
      "sarcasm_recall": 53.4,
      "cost_per_50k": 0.3,
      "developer": "Meta",
      "rank": 29
    },
    {
      "model_name": "openrouter/mistralai/mistral-7b-instruct-v0.3",
      "display_name": "Mistral: Mistral 7B Instruct v0.3",
      "accuracy": 66.5,
      "sarcasm_recall": 46.4,
      "cost_per_50k": 0.45,
      "developer": "Mistral",
      "rank": 30
    },
    {
      "model_name": "openrouter/mistralai/mistral-small-24b-instruct-2501",
      "display_name": "Mistral: Mistral Small 3",
      "accuracy": 65.0,
      "sarcasm_recall": 43.6,
      "cost_per_50k": 0.75,
      "developer": "Mistral",
      "rank": 31
    },
    {
      "model_name": "openrouter/amazon/nova-lite-v1",
      "display_name": "Amazon: Nova Lite 1.0",
      "accuracy": 64.7,
      "sarcasm_recall": 42.4,
      "cost_per_50k": 0.9,
      "developer": "Amazon",
      "rank": 32
    },
    {
      "model_name": "openrouter/cohere/command-r-08-2024",
      "display_name": "Cohere: Command R (08-2024)",
      "accuracy": 64.2,
      "sarcasm_recall": 46.0,
      "cost_per_50k": 2.42,
      "developer": "Cohere",
      "rank": 33
    },
    {
      "model_name": "openrouter/mistralai/mistral-nemo",
      "display_name": "Mistral: Mistral Nemo",
      "accuracy": 61.2,
      "sarcasm_recall": 36.8,
      "cost_per_50k": 0.3,
      "developer": "Mistral",
      "rank": 34
    },
    {
      "model_name": "openrouter/nousresearch/hermes-2-pro-llama-3-8b",
      "display_name": "NousResearch: Hermes 2 Pro - Llama-3 8B",
      "accuracy": 59.1,
      "sarcasm_recall": 33.6,
      "cost_per_50k": 0.4,
      "developer": "Nous Research",
      "rank": 35
    },
    {
      "model_name": "gemini/gemma-3n-e4b-it",
      "display_name": "Gemini: Gemma 3n 4B",
      "accuracy": 57.1,
      "sarcasm_recall": 32.8,
      "cost_per_50k": null,
      "developer": "Google",
      "rank": 36
    },
    {
      "model_name": "openrouter/openai/gpt-4o-mini-2024-07-18",
      "display_name": "OpenAI: GPT-4o-mini (2024-07-18)",
      "accuracy": 55.0,
      "sarcasm_recall": 25.6,
      "cost_per_50k": 2.32,
      "developer": "OpenAI",
      "rank": 37
    },
    {
      "model_name": "openai/gpt-4o-mini",
      "display_name": "OpenAI: GPT-4o-mini",
      "accuracy": 54.5,
      "sarcasm_recall": 25.8,
      "cost_per_50k": 2.32,
      "developer": "OpenAI",
      "rank": 38
    },
    {
      "model_name": "openrouter/microsoft/phi-4",
      "display_name": "Microsoft: Phi 4",
      "accuracy": 52.9,
      "sarcasm_recall": 29.2,
      "cost_per_50k": 0.9,
      "developer": "Microsoft",
      "rank": 39
    },
    {
      "model_name": "openrouter/meta-llama/llama-3-8b-instruct",
      "display_name": "Meta: Llama 3 8B Instruct",
      "accuracy": 52.8,
      "sarcasm_recall": 23.0,
      "cost_per_50k": 0.45,
      "developer": "Meta",
      "rank": 40
    },
    {
      "model_name": "openrouter/qwen/qwen-turbo",
      "display_name": "Qwen: Qwen-Turbo",
      "accuracy": 50.8,
      "sarcasm_recall": 19.0,
      "cost_per_50k": 0.8,
      "developer": "Alibaba",
      "rank": 41
    },
    {
      "model_name": "openrouter/neversleep/llama-3.1-lumimaid-8b",
      "display_name": "NeverSleep: Lumimaid v0.2 8B",
      "accuracy": 45.0,
      "sarcasm_recall": 19.6,
      "cost_per_50k": 1.47,
      "developer": "NeverSleep",
      "rank": 42
    },
    {
      "model_name": "openrouter/baidu/ernie-4.5-21b-a3b",
      "display_name": "Baidu: ERNIE 4.5 21B A3B",
      "accuracy": 44.2,
      "sarcasm_recall": 22.4,
      "cost_per_50k": 1.22,
      "developer": "Baidu",
      "rank": 43
    },
    {
      "model_name": "openrouter/sao10k/l3-lunaris-8b",
      "display_name": "Sao10K: Llama 3 8B Lunaris",
      "accuracy": 42.7,
      "sarcasm_recall": 10.0,
      "cost_per_50k": 0.6,
      "developer": "Sao10K",
      "rank": 44
    },
    {
      "model_name": "openai/gpt-5-nano",
      "display_name": "OpenAI: GPT-5 Nano",
      "accuracy": 40.5,
      "sarcasm_recall": 4.6,
      "cost_per_50k": 1.0,
      "developer": "OpenAI",
      "rank": 45
    },
    {
      "model_name": "openrouter/meta-llama/llama-3.2-3b-instruct",
      "display_name": "Meta: Llama 3.2 3B Instruct",
      "accuracy": 40.3,
      "sarcasm_recall": 4.2,
      "cost_per_50k": 0.3,
      "developer": "Meta",
      "rank": 46
    },
    {
      "model_name": "openrouter/mistralai/ministral-8b",
      "display_name": "Mistral: Ministral 8B",
      "accuracy": 39.5,
      "sarcasm_recall": 2.2,
      "cost_per_50k": 1.47,
      "developer": "Mistral",
      "rank": 47
    },
    {
      "model_name": "openrouter/qwen/qwen-2.5-7b-instruct",
      "display_name": "Qwen2.5 7B Instruct",
      "accuracy": 39.4,
      "sarcasm_recall": 0.0,
      "cost_per_50k": 0.6,
      "developer": "Alibaba",
      "rank": 48
    },
    {
      "model_name": "openrouter/microsoft/phi-3-mini-128k-instruct",
      "display_name": "Microsoft: Phi-3 Mini 128K Instruct",
      "accuracy": 39.2,
      "sarcasm_recall": 4.4,
      "cost_per_50k": 1.72,
      "developer": "Microsoft",
      "rank": 49
    },
    {
      "model_name": "openrouter/mistralai/ministral-3b",
      "display_name": "Mistral: Ministral 3B",
      "accuracy": 39.1,
      "sarcasm_recall": 0.6,
      "cost_per_50k": 0.6,
      "developer": "Mistral",
      "rank": 50
    },
    {
      "model_name": "openrouter/meta-llama/llama-3.2-1b-instruct",
      "display_name": "Meta: Llama 3.2 1B Instruct",
      "accuracy": 38.2,
      "sarcasm_recall": 6.0,
      "cost_per_50k": 0.1,
      "developer": "Meta",
      "rank": 51
    },
    {
      "model_name": "openrouter/liquid/lfm-7b",
      "display_name": "Liquid: LFM 7B",
      "accuracy": 38.1,
      "sarcasm_recall": 2.6,
      "cost_per_50k": 0.15,
      "developer": "Liquid AI",
      "rank": 52
    },
    {
      "model_name": "openrouter/microsoft/phi-3.5-mini-128k-instruct",
      "display_name": "Microsoft: Phi-3.5 Mini 128K Instruct",
      "accuracy": 37.9,
      "sarcasm_recall": 0.8,
      "cost_per_50k": 1.72,
      "developer": "Microsoft",
      "rank": 53
    },
    {
      "model_name": "openrouter/mistralai/mistral-7b-instruct-v0.1",
      "display_name": "Mistral: Mistral 7B Instruct v0.1",
      "accuracy": 37.3,
      "sarcasm_recall": 12.0,
      "cost_per_50k": 1.93,
      "developer": "Mistral",
      "rank": 54
    },
    {
      "model_name": "gemini/gemma-3-4b-it",
      "display_name": "Gemini: Gemma 3 4B",
      "accuracy": 36.3,
      "sarcasm_recall": 1.0,
      "cost_per_50k": null,
      "developer": "Google",
      "rank": 55
    },
    {
      "model_name": "openrouter/arcee-ai/afm-4.5b",
      "display_name": "Arcee AI: AFM 4.5B",
      "accuracy": 34.6,
      "sarcasm_recall": 2.4,
      "cost_per_50k": 1.57,
      "developer": "Arcee AI",
      "rank": 56
    },
    {
      "model_name": "openrouter/cohere/command-r7b-12-2024",
      "display_name": "Cohere: Command R7B (12-2024)",
      "accuracy": 34.5,
      "sarcasm_recall": 0.0,
      "cost_per_50k": 0.6,
      "developer": "Cohere",
      "rank": 57
    },
    {
      "model_name": "openrouter/bytedance/ui-tars-1.5-7b",
      "display_name": "ByteDance: UI-TARS 7B",
      "accuracy": 33.3,
      "sarcasm_recall": 0.0,
      "cost_per_50k": 1.53,
      "developer": "ByteDance",
      "rank": 58
    },
    {
      "model_name": "openrouter/liquid/lfm-3b",
      "display_name": "Liquid: LFM 3B",
      "accuracy": 32.3,
      "sarcasm_recall": 2.8,
      "cost_per_50k": 0.35,
      "developer": "Liquid AI",
      "rank": 59
    }
  ]
}
